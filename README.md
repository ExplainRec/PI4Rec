# Explaining Recommender Systems via Random Path-Integration

The Path-Integration (PI) method is a model-agnostic, post-hoc method which is designed to explain he reasoning behind the recommendations generated by recommender systems.  
By integrating "a path" between a baseline and the input user vector, PI generates an explanation map that illustrates the contribution of each aspect of the userâ€™s data on the recommendation output.
Moreover, we introduce two versions of our method in which the baseline is modeled as a random tensor, undergoing multiple sampling iterations. This process yields a comprehensive set of explanations, facilitating the selection of the best-performing explanation w.r.t. the specific metric of interest.
Using several counterfactual evaluation metrics, our results demonstrate the ,methods capability to provide counterfactual explanations for various recommendation algorithms across different datasets.

## A general overview of the PI method
![LXR_diagram](https://github.com/ExplainRec/PI4Rec/blob/main/PI_diagram.PNG)

## Repository

This repository contains code for the Path-Integration (PI) framework that was evaluated on three publicly available benchmarks, MovieLens1M, a subset of Yahoo!Music dataset and a subset of Pinterest dataset, using three different recommenders, Multi Layer Perceptron (MLP), Variational Auto Encoder (VAE) and Neural Collaborative Filtering (NCF). Hyperparameters optimization was done using optuna.

## Folders

* **processed_data**: contains three subfolders, one for each dataset. In each dataset folder lies the raw data files.
* **code**: contains several code notebooks:
  - data_processing - code related to the preprocessing step for preparing the raw data to run with our models.
  - recommenders_architecture - specifies the architecture of the recommenders that were used in the paper.
  - recommenders_training - contains code related to MLP, VAE and NCF recommenders training.
  - help_functions - includes the framework's functions that are being used in all notebooks.
  - PI_functions - includes the PI method related functions that are being used in the metrics notebooks.
  - PI_base - contains code related to PI base (null user) evaluation.
  - PI_random - contains code related to advanced PI evaluation (random user and random items).
  - metrics - contains code related to baselines evaluation.
  - lime, MLP_SHAP_clusters, VAE_SHAP_clusters, NCF_SHAP_clusters - notebook that contains help functions for calculating the LIME & SHAP baselines

* **checkpoints**: Presently, this folder is empty.  It is the designated location for saving and loading the trained recommender's checkpoints and the trained LXRs.  The checkpoints developed during our project are stored in the 'checkpoints' folder in the attached [drive](https://drive.google.com/drive/u/1/folders/1v8jZD2Ew-D4XA0k1NLxgVsHS6q4Aj-KP).
  
## Requirements

* python ????
* Pytorch 1.8.1
* wandb 0.16.3 (the package we used for monitoring the recommenders training process)

## Usage

To use this code, follow these steps:
+ Create data to work with by running the data_processing notebooks.
  - Or in order to reproduce results from the paper without running the data_processing notebook, please download all files from [here](https://drive.google.com/drive/u/1/folders/1oto5QPrhisx2A4MCwub5OUHYdZTYAQxq) from the relevant folder <dataset_name> to data_preprocessing folder according to the data set you wish to run on. 
+ On every notebook, please specify the "data_name" variable to be 'ML1M'/'Yahoo'/'Pinterest', and the "recommender_name" variable to be 'MLP'/'VAE'/'NCF'.
+ All baselines' were calculated using the 'metrics' notebook, metrics for the PI approach and its variants were calculated using the dedicated notebooks.

